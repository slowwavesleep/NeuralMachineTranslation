## Отчет

### Описание задачи
Задачей данного проекта является автоматический перевод с русского на украинский.

Цель — приблизится по адекватности перевода и значению метрики BLEU к существующей модели (трансформеру), 
[равной **64**](https://github.com/Helsinki-NLP/Tatoeba-Challenge/tree/master/models/rus-ukr).

#### Входные данные
Набор примеров, предварительно разбитый на train, dev, test части. Каждый файл на каждой строке содержит пример на
одном языке. Некоторые примеры достаточно странные. Например, куски предложений без начала или конца. Дополнительную 
предобработку не проводил. Для токенизации использовал `YouTokenToMe`.

#### Формат выходных данных
Для корректного подсчета метрики используется следующий формат записи переводов тестовых примеров, сделанных моделью.

- *<оригинал>*
- *<эталонный перевод>*
- *<перевод модели>*
- *<пустая строка>*

### Обработка данных
Данные Tatoeba Challenge не потрбовалось отдельно обрабатывать.

### Немного о машинном переводе


### Подбор датасета
Начал с подготовки пайплайна для всего процесса обучения. Использовал 
[датасет субтитров](https://github.com/ajinkyakulkarni14/TED-Multilingual-Parallel-Corpus) выступлений TED для перевода
с русского на французский. К сожалению, выяснилось, что в данном датасете довольно плохой перевод. Кроме того, он 
выровнен неправильно, т.е. пары предложений могут сильно отличаться по содержанию.

Затем тестировал [датасет](https://www.manythings.org/anki/) с переводами с русского на английский. Этот датасет, как
мне показалось, более качественный. Таких явных ошибок, как в предыдущем, я не заметил, но они, очевидно, есть. Также,
как и [предупреждают](https://www.manythings.org/corpus/warningtatoeba.html), некоторые предложения довольно
неестественны.

В итоге остановился на [русско-укранском датасете](https://object.pouta.csc.fi/Tatoeba-Challenge/rus-ukr.tar),
входящим в [Tatoeba Challenge](https://github.com/Helsinki-NLP/Tatoeba-Challenge), предлагаемом Helsinki NLP.
Основными критериями выбора были: наличие достаточно большого числа примеров, наличие тестовых данных и посчитанной
на них метрики BLEU, а также наличие возможности самостоятельно оценить адекватность перевода.

### Процесс обучения

Заметно на качество модели повлияло следующее:
- Использование механизма attention
- Использование двунаправленной LSTM в декодере
- Добавление pack padded sequence в энкодере и декодере
- Добавление gradient clipping

Сложно оценить
- Добавление дропаута
- Добавление layer norm
- Добавление функции активации
- Маскирование падов в attention

### Подсчет метрики

### Baseline
Для того, чтобы протестировать построенный пайплайн и оценить минимальный порог качества, использовал простую Encoder
Decoder модель, состоящую из двух LSTM.
Параметры, использованные при обучении модели:


- Размерность эмбеддингов — 256
- Размерность LSTM — 1000
- Spatial Dropout в обеих LSTM — 0.3
- Количество эпох — 5
- Размер батча — 512
- Максимальная длина для обоих языков — 20
- Размер словаря для обоих языков — 7000
- Количество примеров для обучения — 100000

Данная модель показала достаточно низкое качество — 2.1 BLEU. При просмотре самих переводов это, конечно, тоже очевидно.
Во многих случаях перевод вообще никак не связан с входным предложением по содержанию.

*Пример 1*
```
Твой дом большой.
Твій будинок великий.
Температор!
```
*Пример 2*
```
Я боюсь грозы.
Я боюся грози.
Я просто...
```







