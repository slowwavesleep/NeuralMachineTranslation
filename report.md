## Отчёт

### Описание задачи

Задачей данного проекта является автоматический перевод с русского на украинский.

Цель — приблизится по адекватности перевода и значению метрики BLEU к существующей модели (трансформеру), 
[равной **64**](https://github.com/Helsinki-NLP/Tatoeba-Challenge/tree/master/models/rus-ukr).

#### Входные данные

Набор примеров, предварительно разбитый на train, dev, test части. Каждый файл на каждой строке содержит пример на
одном языке. Некоторые примеры достаточно странные. Например, куски предложений без начала или конца. Дополнительную 
предобработку не проводил. Для токенизации использовал `YouTokenToMe`.

#### Формат выходных данных

Для корректного подсчета метрики используется следующий формат записи переводов тестовых примеров, сделанных моделью.

- *<оригинал>*
- *<эталонный перевод>*
- *<перевод модели>*
- *<пустая строка>*

### Обработка данных

Данные Tatoeba Challenge не потребовалось отдельно обрабатывать. Но, поскольку я сначала тестировал другие датасеты,
пришлось подготовить некоторые утилиты для чтения, преобразования в нужный формат и разбиения на train, test, dev.

При необходимости их можно использовать для каких-то других данных.

Какой-либо фильтрации символов или приведения к одному регистру не делал ни для одного из датасетов и, как оказалось,
это не так уж сильно на что-то влияет.

### Немного о машинном переводе

Вообще машинный перевод — это по сути самая первая задача компьютерной лингвистики. Первая система машинного перевода
была представлена еще в 1954 году в ходе Джорджтаунского эксперимента. Система переводила предложения с русского на
английский. Она работала на правилах и ограничивалась узкой предметной областью.


Затем появились системы, основанные на примерах. Из которых развились и системы нейросетевого машинного перевода.
Как таковой, генеративный машинный перевод появился совсем недавно — около 6 лет назад. При этом модель 
энкодер-декодер, передающая от языка-источника контекстный вектор всего предложения, довольно быстро была улучшена
механизмом attention, позволяющим оценить то, насколько важно каждое из слов предложения на языке-источнике для
перевода каждого из слов предложения на языке-цели. Использование этого механизма позволило нейросетевым моделям
превзойти в качестве статистические модели.


Затем появились трансформеры, которые отказались от рекуррентных элементов и стали использовать attention как
основной элемент архитектуры.


Поскольку я хотел с самого начала разобраться с attention, я решил остановиться на модели, использующей LSTM.

### Подбор датасета

Начал с подготовки пайплайна для всего процесса обучения. Использовал 
[датасет субтитров](https://github.com/ajinkyakulkarni14/TED-Multilingual-Parallel-Corpus) выступлений TED для перевода
с русского на французский. К сожалению, выяснилось, что в данном датасете довольно плохой перевод. Кроме того, он 
выровнен неправильно, т.е. пары предложений могут сильно отличаться по содержанию.

Затем тестировал [датасет](https://www.manythings.org/anki/) с переводами с русского на английский. Этот датасет, как
мне показалось, более качественный. Таких явных ошибок, как в предыдущем, я не заметил, но они, очевидно, есть. Также,
как и [предупреждают](https://www.manythings.org/corpus/warningtatoeba.html), некоторые предложения довольно
неестественны.

В итоге остановился на [русско-украинском датасете](https://object.pouta.csc.fi/Tatoeba-Challenge/rus-ukr.tar),
входящим в [Tatoeba Challenge](https://github.com/Helsinki-NLP/Tatoeba-Challenge), предлагаемом Helsinki NLP.
Основными критериями выбора были: наличие достаточно большого числа примеров, наличие тестовых данных и посчитанной
на них метрики BLEU, а также наличие возможности самостоятельно оценить адекватность перевода.

### Подсчет метрики

Метрика BLEU призвана оценивать качество машинного перевода. BLEU может принимать значения от 0 до 100, где
0 — наихудшая оценка, а 100 — наилучшая.


Суть BLEU заключается в сравнении референсных переводов (человеческих) и переводов модели. Сравнивается количество
общих n-граммов, где `n` лежит на промежутке от 1 до 4. Метрика обычно оценивается на всем корпусе. Значение 100
может быть получено только в том случае, если все переводы (человека и машины) полностью совпадают.


У метрики BLEU есть достаточно много проблем, но здесь можно остановиться на двух. Во-первых, BLEU никак не учитывает
использование синонимов, поэтому правильный, но перефразированный перевод будет иметь низкую оценку. Во-вторых, у данной
метрики существует достаточно большое количество настроек и имплементаций, поэтому сравнивать оценки разных систем может
быть достаточно сложно (и потенциально не очень корректно). Поэтому я подобрал датасет, в которым понятно по какому
принципу подсчитан BLEU.


Чтобы получить оценку, которую можно сравнивать использовался пакет [SacreBLEU](https://github.com/mjpost/sacrebleu),
который позволяет единообразно сравнивать и оценивать результаты.


### Процесс обучения и анализ ошибок

Подход к созданию модели был такой. Сначала я сделал бейзлайн модель и посчитатал на ней метрику BLEU.
Затем я стал её последовательно усложнять.


Для обучения всех моделей использовался метод `teacher forcing`, т.е. в декодер сразу передавалась та
последовательность токенов, которую модель и должна была предсказать.


Заметно на качество модели повлияло следующее:
- Использование механизма attention (примерно + 15 к BLEU)
- Использование двунаправленной LSTM в декодере (примерно + 5 к BLEU)
- Добавление pack padded sequence в энкодере и декодере (примерно + 8 к BLEU)
- Добавление gradient clipping (примерно + 1.5 к BLEU)

Сложно оценить влияние по метрикам:
- Добавление дропаута
- Добавление layer norm
- Добавление функции активации
- Маскирование падов в attention

В итоге я обучал 3 модели, которые условно назвал `baseline`, `main`, `colab`. Модели `main` и `colab` имеют одинаковую
архитектуру, но для их обучения было использовано разное количество примеров.

#### Baseline
Для того, чтобы протестировать построенный пайплайн и оценить минимальный порог качества, использовал простую Encoder
Decoder модель, состоящую из двух LSTM.
Параметры, использованные при обучении модели:


- Размерность эмбеддингов — 300
- Размерность LSTM — 1000
- Spatial Dropout в обеих LSTM — 0.3
- Количество эпох — 5
- Размер батча — 512
- Максимальная длина для обоих языков — 20
- Размер словаря для обоих языков — 7000
- Количество примеров для обучения — 100000

Данная модель показала достаточно низкое качество — 2.1 BLEU. При просмотре самих переводов это, конечно, тоже очевидно.
Во многих случаях перевод вообще никак не связан с входным предложением по содержанию.

*Пример 1*
```
Твой дом большой.
Твій будинок великий.
Температор!
```
*Пример 2*
```
Я боюсь грозы.
Я боюся грози.
Я просто...
```


#### Main

Основная модель, которая обучалась локально. Использовалось 100000 примеров, как и для бейзлайн модели.
Был получен результат 32.94 по метрике BLEU.

Ошибок здесь существенно меньше. Можно сказать, что переводы выглядят разумно. Есть ошибки с повторением слов.

*Пример 3*
```
Мэри пила.
Мері пила.
Мері Мері піла.
```

*Пример 4*
```
Мои часы дешевле твоих.
Мій годинник дешевший, ніж твій.
Мої годин частіше твоїх твоїх твоїх твоїх твоїх твоїх твоїх.
```

Также, видимо из-за использования BPE токенизации, модель как будто изобретает несуществующие словоформы.

*Пример 5*
```
Кто украл яблоко?
Хто вкрав яблуко?
Хто прикрав яблуко?
```

Еще встречаются однокоренные слова (при этом не согласованные).

*Пример 6*
```
Я предал тебя.
Я тебе зрадив.
Я віддав тебе.
```

Некоторые предложения переведены вполне правильно, а именно референсный перевод содержит лишнюю информацию.

*Пример 7*
```
Какой маленький телевизор! Он действительно работает?
Який маленький телевізор! А він дійсно працює?
Який маленький телевізор! Він дійсно працює?
```
##### Архитектура

Модель состоит из энкодера, декодера и механизма `Scaled Dot Product Attention`.

Энкодер — двунаправленная LSTM, в которой используются `layer norm`, `spatial dropout`, `sequence packing`.
Скрытые состояние энкодера были конкатенированы для передачи дальше в декодер.

Декодер — однонаправленная LSTM с удвоенной относительно энкодера размерностью, в которой также используются
`layer norm`, `spatial dropout`, `sequence packing`. Модель получает на вход скрытые состояния энкодера.

В attention выходы энкодера и декодера сначала преобразуются с помощью `key`, `value`, `query` проекций. Затем
`query` и `key` матрично перемножаются и делятся на корень из размерности `query`. В полученной матрице
дополнительно маскируются пады энкодера. Далее применяется софтмакс. Затем `query` перемножается на полученную
матрицу. Так получаем часть последовательности энкодера, которую нужно примешать к последовательности декодера.

Далее выход из attention складывается с выходом декодера, затем применяются функция активации `tanh`, `layer norm`,
`spatial dropout`. Дальше последовательность передаётся в полносвязный слой, по выходам которого можно предсказывать
токены.

##### Параметры

При увеличении максимальной длины последовательности до 44 батчи размером 512 перестали влезать в память карты,
поэтому пришлось снизить размер до 256.

Также пробовал использовать разные длины последовательностей для двух языков, но не обнаружил никакого эффекта.

- Размерность эмбеддингов — 256
- Размерность LSTM энкодера — 512
- Размерность LSTM декодера — 1024
- Spatial Dropout в обеих LSTM — 0.3
- Количество эпох — 5
- Размер батча — 256
- Максимальная длина для обоих языков — 44
- Размер словаря для обоих языков — 9000
- Количество примеров для обучения — 100000


#### Colab

Ту же самую модель также обучал на 800000 примерах в колабе, что заняло около 3 часов. В итоге получил 
значение 49.55 BLEU. Батч размером 512 здесь удалось уместить. Также пробовал увеличивать максимальную
длину последовательности, но это оказалось очень дорогой операцией и сильно увеличивало время обучения.

Также пробовал обучать на 200 и 500 тысячах примеров. Во всех случаях метрика BLEU монотонно возрастала
с увеличением количества примеров.

- Размерность эмбеддингов — 256
- Размерность LSTM энкодера — 512
- Размерность LSTM декодера — 1024
- Spatial Dropout в обеих LSTM — 0.3
- Количество эпох — 5
- Размер батча — 512
- Максимальная длина для обоих языков — 44
- Размер словаря для обоих языков — 9000
- Количество примеров для обучения — 800000

##### Ошибки

Здесь можно видеть, что перевод модели ближе к оригиналу, чем перевод человека, но с точки зрения BLEU
это ошибка.


*Пример 8*
```
Он мой учитель.
Це мій вчитель.
Він мій учитель.
```

Перевод правильный, но BLEU не может учесть порядок слов.

*Пример 9*
```
Это яблоко гнилое.
Це гниле яблуко.
Це яблуко гнило.
```

Использование синонимичных словоформ также считается ошибкой.

*Пример 10*
```
Ещё один человек умер.
Ще одна людина вмерла.
Ще один чоловік помер.
```

Вообще найти ошибки, бросающиеся в глаза, оказалось не так-то просто. Но они в основном похожи на пример ниже.

*Пример 11*
```
Он, кстати, ни слова не говорит по-русски.
Він, до речі, ні слова не говорить російською.
Він, до речі, ні слова не говорить про по-руссссссссссссссссссссс
```

Довольно любопытный пример. Кажется, что в некоторых контекстах "немец" и "немецкий" значат одно и то же.
Возможно, модель это выучила.

*Пример 12*
```
Ты тоже немец?
Ти теж німець?
Ти теж німецький?
```

Здесь произошло что-то странное. Должно быть кавычки-ёлочки так действуют на модель.

*Пример 13*
```
«Тебе нравится путешествовать?» — «Да».
"Тобі подобається подорожувати?" - "Так".
« Дерево подобається подорожувати? » — « Дерево подобається? »
```

*Пример 14*
```
«Когда Вы встаёте?» — «В 8 утра».
"Коли ви встаєте?" - "О 8 ранку".
« Ви встаєте? » — « Вісімнадцятирічний.
```
#### Выводы

Видно, что при использовании описанной архитектуры качество растёт с увеличением количества обучающих. Можно предположить,
что при использовании всех данных (около 1.5 миллиона примеров) качество будет еще выше. Пока не стал это проверять, т.к.
обучение на всех данных занимает довольно много времени.


При просмотре переводов лучше модели создаётся впечатление, что модель учится образовывать нужно словоформы
(хоть и не всегда правильно) и использовать синонимы и синонимичные словоформы. Кажется, что модель использует
порядок слов оригинала, но поскольку оба языка имеют свободный порядок слов, влияние порядка определить довольно сложно.
В целом создаётся впечатление, что модель в некоторых случаях генерирует некий гибрид двух языков — суржик.


Выяснилось, что в данных есть некоторое количество не вполне корректных примеров. Кроме того, как мы выше видели,
метрика BLEU не всегда может определить правильный перевод. При всём это модель показывает достаточно хорошие
результаты. Возможно, при оценке модели метриками, лучше оценивающими содержательную составляющую, как например
`ESIM`, основанная на эмбеддингах BERT.

 
